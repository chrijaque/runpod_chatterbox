FROM runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04

# Set environment variables for logging
ENV PYTHONUNBUFFERED=1
ENV PYTHONFAULTHANDLER=1
ENV PYTHON_UNBUFFERED="true"
ENV DEBIAN_FRONTEND=noninteractive

# Optional HF token for private models (pass at build time)
ARG HF_TOKEN=""

# Setup environment for Hugging Face model cache baked into the image
ENV HF_HOME=/models/hf \
    HF_HUB_CACHE=/models/hf/hub \
    TRANSFORMERS_CACHE=/models/hf

# Install system dependencies (including build tools for vLLM compilation)
RUN apt-get update && \
    apt-get install -y \
    git \
    wget \
    curl \
    build-essential \
    ninja-build \
    cmake \
    && rm -rf /var/lib/apt/lists/*

# Set git global config to avoid warnings
RUN git config --global --add safe.directory '*'

# Create requirements files - split into lightweight and heavy dependencies
RUN echo "runpod>=1.5.0" > /requirements_light.txt && \
    echo "requests>=2.31.0" >> /requirements_light.txt && \
    echo "firebase-admin>=6.2.0" >> /requirements_light.txt && \
    echo "transformers>=4.35.0" >> /requirements_light.txt && \
    echo "accelerate>=0.24.0" >> /requirements_light.txt

RUN echo "vllm>=0.6.0" > /requirements_vllm.txt && \
    echo "autoawq>=0.2.0" >> /requirements_vllm.txt

# Add build argument to force cache invalidation
ARG CACHE_BUST=1
ARG BUILD_TIME
ARG RUNPOD_CACHE_BUST=1

# Cache-bust layer
RUN echo "ðŸ§¹ Cache bust: ${CACHE_BUST}-${RUNPOD_CACHE_BUST} at ${BUILD_TIME}" && true

# Install lightweight requirements first (better caching)
RUN echo "ðŸ”§ Installing lightweight requirements..." && \
    pip install --no-cache-dir -r /requirements_light.txt && \
    echo "âœ… Lightweight requirements installed"

# Install AutoAWQ first (faster, always needed as fallback)
RUN echo "ðŸ”§ Installing AutoAWQ..." && \
    pip install --no-cache-dir "autoawq>=0.2.0" && \
    echo "âœ… AutoAWQ installed"

# Install vLLM - try pre-built wheels first, then compile from source if needed
# This ensures vLLM is always available (preferred over AutoAWQ fallback)
RUN echo "ðŸ”§ Installing vLLM..." && \
    export CUDA_HOME=/usr/local/cuda && \
    export PATH=${CUDA_HOME}/bin:${PATH} && \
    export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH} && \
    export VLLM_USE_TRITON_FLASH_ATTN=1 && \
    (echo "ðŸš€ Attempting to install vLLM from pre-built wheels..." && \
     pip install --no-cache-dir --timeout=300 "vllm>=0.6.0,<0.7.0" && \
     echo "âœ… vLLM installed successfully") || \
    (echo "âš ï¸ Pre-built wheel not available, compiling from source..." && \
     echo "â³ This may take 10-30 minutes..." && \
     pip install --no-cache-dir --timeout=3600 "vllm>=0.6.0,<0.7.0" && \
     echo "âœ… vLLM compiled and installed successfully from source")

# Pre-download is optional - model will be loaded from network volume at runtime
# Only pre-download if MODEL_NAME is explicitly set and MODEL_PATH is not set
RUN echo "â„¹ï¸ Model will be loaded from network volume at runtime (MODEL_PATH)" && \
    echo "â„¹ï¸ Set MODEL_NAME environment variable to enable HuggingFace pre-download fallback"

# Copy LLM handler script
COPY handlers/chatterbox/llm_handler.py /

# Expose port (if needed for health checks)
EXPOSE 8000

# Set the entrypoint to the LLM handler
CMD ["python3", "-u", "llm_handler.py"]

