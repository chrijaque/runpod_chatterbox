FROM runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04

# Set environment variables for logging
ENV PYTHONUNBUFFERED=1
ENV PYTHONFAULTHANDLER=1
ENV PYTHON_UNBUFFERED="true"
ENV DEBIAN_FRONTEND=noninteractive

# Optional HF token for private models (pass at build time)
ARG HF_TOKEN=""

# Setup environment for Hugging Face model cache baked into the image
ENV HF_HOME=/models/hf \
    HF_HUB_CACHE=/models/hf/hub \
    TRANSFORMERS_CACHE=/models/hf

# Install system dependencies
RUN apt-get update && \
    apt-get install -y \
    git \
    wget \
    curl && \
    rm -rf /var/lib/apt/lists/*

# Set git global config to avoid warnings
RUN git config --global --add safe.directory '*'

# Create requirements file for LLM handler
RUN echo "runpod>=1.5.0" > /requirements_llm.txt && \
    echo "vllm>=0.6.0" >> /requirements_llm.txt && \
    echo "transformers>=4.35.0" >> /requirements_llm.txt && \
    echo "accelerate>=0.24.0" >> /requirements_llm.txt && \
    echo "requests>=2.31.0" >> /requirements_llm.txt && \
    echo "autoawq>=0.2.0" >> /requirements_llm.txt && \
    echo "firebase-admin>=6.2.0" >> /requirements_llm.txt

# Add build argument to force cache invalidation
ARG CACHE_BUST=1
ARG BUILD_TIME
ARG RUNPOD_CACHE_BUST=1

# Cache-bust layer
RUN echo "üßπ Cache bust: ${CACHE_BUST}-${RUNPOD_CACHE_BUST} at ${BUILD_TIME}" && true

# Install requirements (PyTorch is already in base image)
RUN echo "üîß Installing LLM requirements..." && \
    pip install -r /requirements_llm.txt && \
    echo "‚úÖ LLM requirements installed"

# Pre-download is optional - model will be loaded from network volume at runtime
# Only pre-download if MODEL_NAME is explicitly set and MODEL_PATH is not set
RUN echo "‚ÑπÔ∏è Model will be loaded from network volume at runtime (MODEL_PATH)" && \
    echo "‚ÑπÔ∏è Set MODEL_NAME environment variable to enable HuggingFace pre-download fallback"

# Copy LLM handler script
COPY handlers/chatterbox/llm_handler.py /

# Expose port (if needed for health checks)
EXPOSE 8000

# Set the entrypoint to the LLM handler
CMD ["python3", "-u", "llm_handler.py"]

