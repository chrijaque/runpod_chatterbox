name = "chatterbox-llm"
description = "Chatterbox LLM Story Generation Handler (Qwen2.5-32B-Instruct-AWQ with vLLM)"
repository = "https://github.com/chrijaque/runpod_chatterbox"
docker_image_name = "chatterbox-llm"
docker_image_tag = "prod"

[env]
RUNPOD_API_KEY = "{{ RUNPOD_API_KEY }}"
MINSTRALY_API_SHARED_SECRET = "{{ MINSTRALY_API_SHARED_SECRET }}"
FIREBASE_SERVICE_ACCOUNT = "{{ FIREBASE_SERVICE_ACCOUNT }}"  # Firebase service account JSON
MODEL_PATH = "/runpod-volume/models/Qwen2.5-32B-Instruct-AWQ"  # Network volume path
MODEL_NAME = "Qwen2.5-32B-Instruct-AWQ"  # Fallback HuggingFace model if MODEL_PATH not found
CACHE_BUST = "prod-{{ timestamp }}"

[build]
dockerfile = "dockerfiles/chatterbox/Dockerfile.llm"
context = "."
build_args = ["RUNPOD_CACHE_BUST={{ CACHE_BUST }}", "HF_TOKEN={{ HF_TOKEN }}"]

[deploy]
gpu_type = "RTX 3090"  # Can use smaller GPU than TTS since no audio processing
container_disk_in_gb = 30  # Model size dependent (~20-30GB for Qwen 2.5)
volume_in_gb = 50  # Network volume size - adjust based on model size
volume_mount_path = "/runpod-volume"  # Mount point for network volume
ports = "8000/http"
envs = [
    "RUNPOD_API_KEY",
    "MINSTRALY_API_SHARED_SECRET",
    "FIREBASE_SERVICE_ACCOUNT",
    "MODEL_PATH",
    "MODEL_NAME",
    "VERBOSE_LOGS"
]

